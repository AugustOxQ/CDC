{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from datasets import config, load_dataset\n",
    "from PIL import Image, ImageFile\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True  # To handle truncated (corrupted) images\n",
    "\n",
    "custom_download_path = \"/data/SSD2/HF_datasets\"\n",
    "config.HF_DATASETS_CACHE = custom_download_path\n",
    "\n",
    "from transformers import AutoImageProcessor, AutoTokenizer\n",
    "\n",
    "preprocess = AutoImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CDC_test(Dataset):\n",
    "\n",
    "    def __init__(self, annotation_path, image_path, preprocess, ratio=0.1):\n",
    "        self.annotations = json.load(open(annotation_path))\n",
    "        self.annotations = self.annotations[: int(len(self.annotations) * ratio)]\n",
    "        self.image_path = image_path\n",
    "        self.vis_processors = preprocess\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        annotation = self.annotations[idx]\n",
    "        img_path = os.path.join(self.image_path, annotation[\"image\"])\n",
    "        raw_image = Image.open(img_path).convert(\"RGB\")\n",
    "        image_input = self.vis_processors(raw_image, return_tensors=\"pt\")\n",
    "        if \"pixel_values\" in image_input:\n",
    "            image_input[\"pixel_values\"] = image_input[\"pixel_values\"].squeeze()\n",
    "\n",
    "        raw_text = (\n",
    "            self.annotations[idx][\"caption\"]\n",
    "            # if type(self.annotations[idx][\"caption\"]) == str\n",
    "            # else self.annotations[idx][\"caption\"][0]\n",
    "        )\n",
    "\n",
    "        return image_input, raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/data/SSD/flickr30k/annotations/test.json\"\n",
    "annotation = json.load(open(path, \"r\"))\n",
    "\n",
    "test_dataset = CDC_test(\n",
    "    annotation_path=path,\n",
    "    image_path=\"/data/SSD/flickr30k/images\",\n",
    "    preprocess=preprocess,\n",
    "    ratio=1,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1024,\n",
    "    shuffle=False,\n",
    "    # num_workers=cfg.train.num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_txt = \"/project/Deep-Clustering/res/20240717_102331_flickr30k-preextracted/all_txt_emb_21.pt\"\n",
    "path_img = \"/project/Deep-Clustering/res/20240717_102331_flickr30k-preextracted/all_img_emb_21.pt\"\n",
    "path_comb = (\n",
    "    \"/project/Deep-Clustering/res/20240717_102331_flickr30k-preextracted/all_best_comb_emb_21.pt\"\n",
    ")\n",
    "\n",
    "txt_emb = torch.tensor(torch.load(path_txt))\n",
    "img_emb = torch.tensor(torch.load(path_img))\n",
    "comb_emb = torch.tensor(torch.load(path_comb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dask.array as da\n",
    "\n",
    "# Perform k-means\n",
    "# import numpy as np\n",
    "import torch\n",
    "\n",
    "# from cuml import DBSCAN, HDBSCAN, KMeans\n",
    "# from cuml.cluster import DBSCAN, HDBSCAN, KMeans\n",
    "# from cuml.dask.manifold import UMAP as MNMG_UMAP\n",
    "# from cuml.datasets import make_blobs\n",
    "from cuml.manifold import UMAP\n",
    "from dask.distributed import Client\n",
    "from dask_cuda import LocalCUDACluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clustering:\n",
    "    def __init__(self, embedding_manager, device=\"cuda\"):\n",
    "        self.embedding_manager = embedding_manager\n",
    "        self.device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
    "        self.cluster = None\n",
    "        self.client = None\n",
    "\n",
    "    def initialize_cluster(self):\n",
    "        # Initialize Dask CUDA cluster and client\n",
    "        self.cluster = LocalCUDACluster(threads_per_worker=1)\n",
    "        self.client = Client(self.cluster)\n",
    "\n",
    "    def close_cluster(self):\n",
    "        # Close Dask CUDA cluster and client\n",
    "        self.client.close()\n",
    "        self.cluster.close()\n",
    "\n",
    "    def get_umap(self, label_embedding):\n",
    "        # Perform UMAP dimensionality reduction on embeddings\n",
    "        self.initialize_cluster()\n",
    "\n",
    "        label_embedding = label_embedding.to(self.device)\n",
    "        label_embedding_np = label_embedding.cpu().numpy()\n",
    "\n",
    "        local_model = UMAP(random_state=42)\n",
    "        umap_features = local_model.fit_transform(label_embedding_np)\n",
    "\n",
    "        self.close_cluster()\n",
    "\n",
    "        umap_features = torch.tensor(umap_features, device=self.device)\n",
    "        return umap_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = Clustering(embedding_manager=None, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_txt = clustering.get_umap(txt_emb)\n",
    "umap_img = clustering.get_umap(img_emb)\n",
    "umap_comb = clustering.get_umap(comb_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot UMAP\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "umap_txt_np = umap_txt.cpu().numpy()\n",
    "umap_img_np = umap_img.cpu().numpy()\n",
    "umap_comb_np = umap_comb.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.scatter(umap_txt_np[:, 0], umap_txt_np[:, 1], s=0.1, c=\"r\", label=\"Text\")\n",
    "plt.scatter(umap_img_np[:, 0], umap_img_np[:, 1], s=0.1, c=\"g\", label=\"Image\")\n",
    "\n",
    "x, y = umap_img_np[1, :]\n",
    "plt.scatter(x, y, c=\"red\", s=150, edgecolors=\"k\")  # Highlight the sample\n",
    "plt.text(x, y, f\"Sample {0} img\", fontsize=10, color=\"black\")  # Annotate the sample\n",
    "\n",
    "x, y = umap_txt_np[1, :]\n",
    "plt.scatter(x, y, c=\"red\", s=150, edgecolors=\"k\")  # Highlight the sample\n",
    "plt.text(x, y, f\"Sample {0} txt\", fontsize=10, color=\"black\")  # Annotate the sample\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.scatter(umap_comb_np[:, 0], umap_comb_np[:, 1], s=0.1, c=\"b\", label=\"Combined\")\n",
    "plt.scatter(umap_img_np[:, 0], umap_img_np[:, 1], s=0.1, c=\"g\", label=\"Image\")\n",
    "\n",
    "x, y = umap_img_np[1, :]\n",
    "plt.scatter(x, y, c=\"red\", s=150, edgecolors=\"k\")  # Highlight the sample\n",
    "plt.text(x, y, f\"Sample {0} img\", fontsize=10, color=\"black\")  # Annotate the sample\n",
    "\n",
    "x, y = umap_comb_np[1, :]\n",
    "plt.scatter(x, y, c=\"red\", s=150, edgecolors=\"k\")  # Highlight the sample\n",
    "plt.text(x, y, f\"Sample {0} comb\", fontsize=10, color=\"black\")  # Annotate the sample\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepclustering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
