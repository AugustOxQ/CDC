{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, CLIPModel\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Vision Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(\n",
    "    text=[\"a photo of a cat\", \"a photo of a dog\"],\n",
    "    images=image,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    ").to(device)\n",
    "\n",
    "image_input = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "text_input = processor(\n",
    "    text=[\"a photo of a cat\", \"a photo of a dog\"],\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    ").to(device)\n",
    "\n",
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n",
    "logit_scale = model.logit_scale.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeds = outputs.image_embeds\n",
    "text_embeds = outputs.text_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeds_norm = image_embeds / image_embeds.norm(dim=-1, keepdim=True)\n",
    "text_embeds_norm = text_embeds / text_embeds.norm(dim=-1, keepdim=True)\n",
    "similarity = (100.0 * image_embeds_norm @ text_embeds_norm.T).softmax(dim=-1) * logit_scale\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logits_per_image, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_vm = model.vision_model\n",
    "clip_vproj = model.visual_projection\n",
    "\n",
    "clip_tm = model.text_model\n",
    "clip_tproj = model.text_projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embed_single = clip_vm(**image_input).pooler_output\n",
    "text_embed_single = clip_tm(**text_input).pooler_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embed_single_proj = clip_vproj(image_embed_single)\n",
    "text_embed_single_proj = clip_tproj(text_embed_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity2 = (100.0 * image_embed_single_proj @ text_embed_single_proj.T).softmax(\n",
    "    dim=-1\n",
    ") * logit_scale\n",
    "print(similarity2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features = model.get_image_features(**image_input)\n",
    "image_features.shape\n",
    "\n",
    "# s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(image_features == image_embeds).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(image_features == image_embed_single_proj).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以这里能看出，model.get_image_features所获得的，和我先进入vmodel再proj pooled_output得到的是一样的，所以我的方法是对的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with annotation file and umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_path = \"/project/Deep-Clustering/data/flickr30k/test.json\"\n",
    "img_path = \"/data/SSD/flickr30k/images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractionDataset(Dataset):\n",
    "    def __init__(self, annotation_path: str, image_path: str, processor, ratio=0.1) -> None:\n",
    "        self.annotations = json.load(open(annotation_path))\n",
    "        self.annotations = self.annotations[: int(len(self.annotations) * ratio)]\n",
    "        self.image_path = image_path\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple:\n",
    "        annotation = self.annotations[idx]\n",
    "        img_path = os.path.join(self.image_path, annotation[\"image\"])\n",
    "        raw_image = Image.open(img_path).convert(\"RGB\")\n",
    "        image_input = self.processor(images=raw_image, return_tensors=\"pt\")\n",
    "\n",
    "        if \"pixel_values\" in image_input:\n",
    "            image_input[\"pixel_values\"] = image_input[\"pixel_values\"].squeeze()\n",
    "\n",
    "        raw_text = (\n",
    "            self.annotations[idx][\"caption\"]\n",
    "            if type(self.annotations[idx][\"caption\"]) is str\n",
    "            else self.annotations[idx][\"caption\"][0]\n",
    "        )\n",
    "\n",
    "        return image_input, raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = FeatureExtractionDataset(ann_path, img_path, processor, ratio=1)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_embeddings, text_embeddings = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader):\n",
    "        images, raw_texts = batch\n",
    "        image_input = images.to(device)\n",
    "        text_input = processor(\n",
    "            text=raw_texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=77,\n",
    "        ).to(device)\n",
    "\n",
    "        img_embed = clip_vm(**image_input).pooler_output\n",
    "        text_embed = clip_tm(**text_input).pooler_output\n",
    "\n",
    "        img_embed = clip_vproj(img_embed)\n",
    "        text_embed = clip_tproj(text_embed)\n",
    "\n",
    "        img_embeddings.append(img_embed)\n",
    "        text_embeddings.append(text_embed)\n",
    "\n",
    "img_embeddings = torch.cat(img_embeddings)\n",
    "text_embeddings = torch.cat(text_embeddings)\n",
    "\n",
    "print(img_embeddings.shape, text_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from cuml.cluster import HDBSCAN, KMeans\n",
    "from cuml.dask.manifold import UMAP as MNMG_UMAP\n",
    "from cuml.datasets import make_blobs\n",
    "from cuml.manifold import UMAP\n",
    "from dask.distributed import Client\n",
    "from dask_cuda import LocalCUDACluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCUDACluster(threads_per_worker=1)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_embedding = text_embeddings.clone().detach().to(device)\n",
    "label_embedding = label_embedding.to(device)\n",
    "label_embedding_np = label_embedding.cpu().numpy()\n",
    "\n",
    "local_model = UMAP(random_state=42, n_components=2)\n",
    "umap_features = local_model.fit_transform(label_embedding_np)\n",
    "\n",
    "umap_features = torch.tensor(umap_features, device=\"cpu\")\n",
    "umap_features_np = umap_features.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(umap_features[:, 0], umap_features[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepclustering2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
