{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from tabnanny import verbose\n",
    "from turtle import update\n",
    "\n",
    "import hydra\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "from hydra import compose, initialize, initialize_config_dir, initialize_config_module\n",
    "from IPython.display import clear_output, display\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from sklearn.cluster import KMeans\n",
    "from sympy import count_ops, use\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoImageProcessor, AutoProcessor, AutoTokenizer\n",
    "\n",
    "# Import local packages\n",
    "from src.data.cdc_datamodule import CDC_test\n",
    "from src.models.cdc import CDC\n",
    "from src.models.components.clustering import Clustering, UMAP_vis\n",
    "from src.utils import (\n",
    "    print_model_info,\n",
    ")\n",
    "from src.utils.evaltools import eval_rank_oracle_check_per_label\n",
    "from src.utils.inference import encode_data, inference_test\n",
    "\n",
    "# Setup\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "transformers.logging.set_verbosity_error()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize Hydra\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "initialize(config_path=\"configs\", version_base=None)\n",
    "cfg = compose(config_name=\"flickr30k\")\n",
    "print(*cfg, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "from eval import main\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)  # Ensures full text is shown\n",
    "pd.set_option(\"display.max_rows\", 200)  # Increase max rows if needed\n",
    "pd.set_option(\"display.max_columns\", 50)  # Increase max columns if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "Select the best label for each image and text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This corresponds to the main() function in the original code: eval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "seed = cfg.seed\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Define the parent folder\n",
    "parent_folder = \"res\"\n",
    "\n",
    "res_path = \"/project/Deep-Clustering/res/20250407_175254_flickr30k-preextracted\"\n",
    "\n",
    "if res_path is None:\n",
    "    print(\"No path provided. Searching for the latest experiment...\")\n",
    "    # Get a list of all subdirectories inside the parent folder\n",
    "    subfolders = [\n",
    "        os.path.join(parent_folder, d)\n",
    "        for d in os.listdir(parent_folder)\n",
    "        if os.path.isdir(os.path.join(parent_folder, d))\n",
    "    ]\n",
    "\n",
    "    # Sort subfolders by modification time (newest first)\n",
    "    res_path = max(subfolders, key=os.path.getmtime) if subfolders else None\n",
    "\n",
    "print(f\"Using results from: {res_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_best_label = True\n",
    "\n",
    "# Initialize Model\n",
    "model = CDC(\n",
    "    clip_trainable=False,\n",
    "    d_model=cfg.model.d_model,\n",
    "    nhead=cfg.model.num_heads,\n",
    "    num_layers=cfg.model.num_layers,\n",
    "    label_dim=cfg.model.label_dim,\n",
    ")\n",
    "model = nn.DataParallel(model)\n",
    "# load model\n",
    "model.load_state_dict(torch.load(f\"{res_path}/final_model.pth\"))\n",
    "model.to(device)\n",
    "\n",
    "clustering = Clustering()\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "ann_path = cfg.dataset.test_path\n",
    "ann = json.load(open(ann_path, \"r\"))\n",
    "\n",
    "if len(ann) > 5000:\n",
    "    ratio = 5000 / len(ann)\n",
    "else:\n",
    "    ratio = 1\n",
    "\n",
    "del ann_path, ann\n",
    "\n",
    "test_dataset = CDC_test(\n",
    "    annotation_path=cfg.dataset.test_path,\n",
    "    image_path=cfg.dataset.img_path_test,\n",
    "    processor=processor,\n",
    "    ratio=ratio,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=cfg.eval.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=cfg.train.num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXP 2: Test all labels and focus visualization of a single image / text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"##########Testing test dataset##########\")\n",
    "unique_embeddings = torch.load(f\"{res_path}/unique_embeddings.pt\")\n",
    "(\n",
    "    img_emb,\n",
    "    txt_emb,\n",
    "    txt_full,\n",
    "    text_to_image_map,\n",
    "    image_to_text_map,\n",
    "    inds_raw_tti,\n",
    "    inds_raw_itt,\n",
    ") = encode_data(\n",
    "    model,\n",
    "    processor,\n",
    "    test_dataloader,\n",
    "    device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go with itt experiments through all labels for a single image\n",
    "inds_itt_all = []\n",
    "mask_itt_all = []\n",
    "for selected_label in tqdm(unique_embeddings):\n",
    "    (\n",
    "        _,\n",
    "        _,\n",
    "        _,\n",
    "        inds_itt,\n",
    "        mask_itt,\n",
    "    ) = eval_rank_oracle_check_per_label(\n",
    "        model,\n",
    "        selected_label,\n",
    "        img_emb,\n",
    "        txt_emb,\n",
    "        txt_full,\n",
    "        text_to_image_map,\n",
    "        image_to_text_map,\n",
    "        inds_raw_tti=inds_raw_tti,\n",
    "        inds_raw_itt=inds_raw_itt,\n",
    "    )\n",
    "\n",
    "    inds_itt_all.append(inds_itt)\n",
    "    mask_itt_all.append(mask_itt)\n",
    "\n",
    "\"\"\"\n",
    "1. inds_itt is the indices of the itt inds using combined embeddings\n",
    "2. mask_itt is the mask of the image-shape that indicate which image improved by using the selected label.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization itt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_path = cfg.dataset.test_path\n",
    "img_path = cfg.dataset.img_path_test\n",
    "\n",
    "ann = json.load(open(ann_path, \"r\"))\n",
    "txt_collection = [item[\"caption\"] for item in ann]\n",
    "\n",
    "if type(txt_collection[0]) is not str:\n",
    "    txt_collection = [item for sublist in txt_collection for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_index = 10  # Which image to choose to visualize\n",
    "mask_itt_indices = 0  # Which label to choose to visualize\n",
    "only_see_improved = False  # Only see mask_itt_indices if it improved the image\n",
    "check_top_k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_index_slider = widgets.IntSlider(\n",
    "    value=image_index, min=0, max=len(ann) - 1, step=1, description=\"Image Index\"\n",
    ")\n",
    "mask_itt_slider = widgets.IntSlider(\n",
    "    value=mask_itt_indices,\n",
    "    min=0,\n",
    "    max=len(unique_embeddings),\n",
    "    step=1,\n",
    "    description=\"Label Index\",\n",
    ")\n",
    "only_improved_checkbox = widgets.Checkbox(\n",
    "    value=only_see_improved, description=\"Only Show Improved\"\n",
    ")\n",
    "top_k_slider = widgets.IntSlider(value=check_top_k, min=1, max=50, step=1, description=\"Top K\")\n",
    "\n",
    "\n",
    "def update_visualization(image_index, mask_itt_indices, only_see_improved, check_top_k):\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    inds_itt_per_image = [inds_itt[image_index] for inds_itt in inds_itt_all]\n",
    "    mask_itt_per_image = [mask_itt[image_index] for mask_itt in mask_itt_all]\n",
    "\n",
    "    # Make sure that image_index and mask_itt_indices are within the range\n",
    "    image_index = min(image_index, len(img_emb) - 1)\n",
    "    mask_itt_indices = min(mask_itt_indices, len(mask_itt_per_image) - 1)\n",
    "\n",
    "    # Find the True values in mask_itt and its corresponding indices\n",
    "    mask_itt_indices_true = np.where(mask_itt_per_image)[0]\n",
    "    # print(\n",
    "    #     f\"Image is improved with at least one label: {len(mask_itt_indices_true) > 0}\"\n",
    "    # )\n",
    "    # print(\n",
    "    #     f\"Number of labels that improved the image: {len(mask_itt_indices_true)} out of {len(mask_itt_per_image)}\"\n",
    "    # )\n",
    "    improved = mask_itt_per_image[mask_itt_indices]\n",
    "\n",
    "    if only_see_improved and improved:\n",
    "        mask_itt_indices = mask_itt_indices_true[\n",
    "            min(mask_itt_indices, len(mask_itt_indices_true) - 1)\n",
    "        ]\n",
    "\n",
    "    # Apply filtering\n",
    "    if only_see_improved and not improved:\n",
    "        print(\"No improvement for this image.\")\n",
    "        return\n",
    "\n",
    "    # Get item\n",
    "    item = ann[image_index]\n",
    "\n",
    "    img = os.path.join(img_path, item[\"image\"])\n",
    "    img = Image.open(img).convert(\"RGB\")\n",
    "\n",
    "    # turn off axis\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Improved\" if improved else \"Not Improved\")\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "    # First get original caption\n",
    "    original_caption = item[\"caption\"]\n",
    "\n",
    "    # Then get the caption retrieved by raw\n",
    "    retrived_caption_index_raw = inds_raw_itt[image_index][:check_top_k]\n",
    "    retrived_caption_raw = [txt_collection[i] for i in retrived_caption_index_raw]\n",
    "\n",
    "    # Finally get the caption retrieved by our method\n",
    "    retrived_caption_index_cdc = inds_itt_per_image[mask_itt_indices][:check_top_k].tolist()\n",
    "    retrived_caption_cdc = [txt_collection[i] for i in retrived_caption_index_cdc]\n",
    "\n",
    "    # Turn into a panda dataframe\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"Raw_retrieve\": retrived_caption_raw,\n",
    "            \"CDC_retrieve\": retrived_caption_cdc,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Function to highlight duplicates\n",
    "    def highlight_duplicates(val, col1, col2):\n",
    "        # If the value appears in both columns, color it blue\n",
    "\n",
    "        if val in original_caption:\n",
    "            return \"background-color: lightgreen\"\n",
    "\n",
    "        if val in df[col1].values and val in df[col2].values:\n",
    "            return \"background-color: lightblue\"\n",
    "\n",
    "        return \"\"\n",
    "\n",
    "    # Display the dataframe\n",
    "    with pd.option_context(\"display.max_colwidth\", None):\n",
    "        styled_df = df.style.map(highlight_duplicates, col1=\"Raw_retrieve\", col2=\"CDC_retrieve\")\n",
    "        display(styled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive UI\n",
    "ui = widgets.VBox([image_index_slider, mask_itt_slider, only_improved_checkbox, top_k_slider])\n",
    "\n",
    "out = widgets.Output()\n",
    "\n",
    "\n",
    "def on_change(change):\n",
    "    with out:\n",
    "        update_visualization(\n",
    "            image_index_slider.value,\n",
    "            mask_itt_slider.value,\n",
    "            only_improved_checkbox.value,\n",
    "            top_k_slider.value,\n",
    "        )\n",
    "\n",
    "\n",
    "image_index_slider.observe(on_change, names=\"value\")\n",
    "mask_itt_slider.observe(on_change, names=\"value\")\n",
    "only_improved_checkbox.observe(on_change, names=\"value\")\n",
    "top_k_slider.observe(on_change, names=\"value\")\n",
    "\n",
    "update_visualization(image_index, mask_itt_indices, only_see_improved, check_top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ui, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepclustering2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
