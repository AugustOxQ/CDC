{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from tabnanny import verbose\n",
    "from turtle import update\n",
    "from typing import List\n",
    "\n",
    "import hydra\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "from hydra import compose, initialize, initialize_config_dir, initialize_config_module\n",
    "from IPython.display import clear_output, display\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.cluster import KMeans\n",
    "from sympy import count_ops, use\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoImageProcessor, AutoProcessor, AutoTokenizer\n",
    "\n",
    "# Import local packages\n",
    "from src.data.cdc_datamodule import CDC_test\n",
    "from src.models.cdc import CDC\n",
    "from src.models.components.clustering import Clustering, UMAP_vis\n",
    "from src.utils import (\n",
    "    EmbeddingManager,\n",
    "    print_model_info,\n",
    ")\n",
    "from src.utils.evaltools import eval_rank_oracle_check_per_label\n",
    "from src.utils.inference import encode_data, inference_test\n",
    "\n",
    "# Setup\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "transformers.logging.set_verbosity_error()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize Hydra\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "initialize(config_path=\"configs\", version_base=None)\n",
    "cfg = compose(config_name=\"redcaps\")\n",
    "print(*cfg, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "from eval import main\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)  # Ensures full text is shown\n",
    "pd.set_option(\"display.max_rows\", 200)  # Increase max rows if needed\n",
    "pd.set_option(\"display.max_columns\", 50)  # Increase max columns if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_umap(umap_features_np, umap_labels, cluster_centers, representatives):\n",
    "    # Plot UMAP before clustering update\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    tmp_labels = umap_labels >= 0\n",
    "\n",
    "    if umap_features_np is not None:\n",
    "        plt.scatter(\n",
    "            umap_features_np[~tmp_labels, 0],\n",
    "            umap_features_np[~tmp_labels, 1],\n",
    "            c=[0.5, 0.5, 0.5],\n",
    "            s=0.2,\n",
    "            alpha=0.5,\n",
    "        )\n",
    "\n",
    "        plt.scatter(\n",
    "            umap_features_np[tmp_labels, 0],\n",
    "            umap_features_np[tmp_labels, 1],\n",
    "            c=umap_labels[tmp_labels],\n",
    "            s=0.2,\n",
    "            alpha=0.5,\n",
    "        )\n",
    "\n",
    "    if cluster_centers is not None:\n",
    "        plt.scatter(\n",
    "            cluster_centers[:, 0],\n",
    "            cluster_centers[:, 1],\n",
    "            c=\"black\",\n",
    "            s=100,\n",
    "            marker=\"x\",\n",
    "            label=\"Cluster Centers\",\n",
    "        )\n",
    "\n",
    "    if representatives is not None:\n",
    "        plt.scatter(\n",
    "            representatives[:, 0],\n",
    "            representatives[:, 1],\n",
    "            c=\"red\",\n",
    "            s=100,\n",
    "            marker=\"o\",\n",
    "            label=\"Representatives\",\n",
    "        )\n",
    "        labels = np.arange(len(representatives))\n",
    "        for i, (xi, yi) in enumerate(zip(representatives[:, 0], representatives[:, 1])):\n",
    "            plt.text(xi + 0.4, yi + 0.4, str(labels[i]), fontsize=12, color=\"purple\")\n",
    "\n",
    "    # Add the number of umap_labels to the plot as title\n",
    "    plt.title(\"UMAP with cluster_centers\")\n",
    "    plt.colorbar()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "\n",
    "Select the best label for each image and text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This corresponds to the main() function in the original code: eval.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "seed = cfg.seed\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Define the parent folder\n",
    "parent_folder = \"res\"\n",
    "\n",
    "res_path = None\n",
    "\n",
    "if res_path is None:\n",
    "    print(\"No path provided. Searching for the latest experiment...\")\n",
    "    # Get a list of all subdirectories inside the parent folder\n",
    "    subfolders = [\n",
    "        os.path.join(parent_folder, d)\n",
    "        for d in os.listdir(parent_folder)\n",
    "        if os.path.isdir(os.path.join(parent_folder, d))\n",
    "    ]\n",
    "\n",
    "    # Sort subfolders by modification time (newest first)\n",
    "    res_path = max(subfolders, key=os.path.getmtime) if subfolders else None\n",
    "\n",
    "print(f\"Using results from: {res_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_best_label = True\n",
    "\n",
    "# Initialize Model\n",
    "model = CDC(\n",
    "    clip_trainable=False,\n",
    "    d_model=cfg.model.d_model,\n",
    "    nhead=cfg.model.num_heads,\n",
    "    num_layers=cfg.model.num_layers,\n",
    "    label_dim=cfg.model.label_dim,\n",
    ")\n",
    "model = nn.DataParallel(model)\n",
    "# load model\n",
    "model.load_state_dict(torch.load(f\"{res_path}/final_model.pth\"))\n",
    "model.to(device)\n",
    "\n",
    "clustering = Clustering()\n",
    "umap_vis = UMAP_vis()\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_path = cfg.dataset.test_path\n",
    "ann = json.load(open(ann_path, \"r\"))\n",
    "\n",
    "if len(ann) > 5000:\n",
    "    ratio = 5000 / len(ann)\n",
    "else:\n",
    "    ratio = 1\n",
    "print(ratio)\n",
    "\n",
    "test_dataset = CDC_test(\n",
    "    annotation_path=cfg.dataset.test_path,\n",
    "    image_path=cfg.dataset.img_path_test,\n",
    "    processor=processor,\n",
    "    ratio=ratio,\n",
    "    crop_num=5,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=cfg.eval.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=cfg.train.num_workers,\n",
    ")\n",
    "\n",
    "unique_embeddings = torch.load(f\"{res_path}/unique_embeddings.pt\")\n",
    "\n",
    "store_path_0 = \"/project/Deep-Clustering/ckpt2/tmp0\"  # Store np features of all\n",
    "if not os.path.exists(store_path_0):\n",
    "    os.makedirs(store_path_0)\n",
    "\n",
    "\n",
    "store_path = \"/project/Deep-Clustering/ckpt2/tmp\"\n",
    "# Check if the directory exists, if not, create it\n",
    "if not os.path.exists(store_path):\n",
    "    os.makedirs(store_path)\n",
    "store_path_2 = \"/project/Deep-Clustering/ckpt2/tmp2\"\n",
    "# Check if the directory exists, if not, create it\n",
    "if not os.path.exists(store_path_2):\n",
    "    os.makedirs(store_path_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check item in test_dataset\n",
    "item = test_dataset[2]\n",
    "print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## if extracted already, just load the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_manager = EmbeddingManager(\n",
    "    ann,\n",
    "    embedding_dim=cfg.model.label_dim,\n",
    "    chunk_size=cfg.train.batch_size,\n",
    "    embeddings_dir=f\"{res_path}/init/\",\n",
    "    load_existing=True,\n",
    "    sample_ids_list=None,\n",
    ")\n",
    "all_embeddings = embedding_manager.get_all_embeddings()\n",
    "sample_ids, label_embedding = embedding_manager.get_all_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_features = umap_vis.learn_umap(label_embedding, n_components=2)\n",
    "\n",
    "print(\"##########Performing Clustering##########\")\n",
    "umap_labels, _ = clustering.get_hdbscan(umap_features, n_clusters=0, method=\"leaf\")\n",
    "umap_features_cluster = umap_vis.predict_umap(unique_embeddings.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If want to re-compute the center in high dimensional space, load that block. That happen if the clustering center is changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cluster_centers, cluster_counts = clustering.hdbscan_update(\n",
    "    umap_labels=umap_labels,\n",
    "    original_embeddings=label_embedding,\n",
    "    update_type=\"hard\",\n",
    "    alpha=0.4,\n",
    "    update_noise=\"assign\",\n",
    "    center_only=True,\n",
    ")\n",
    "center_sorted_indices = torch.argsort(cluster_counts, descending=True)\n",
    "unique_embeddings = cluster_centers[center_sorted_indices]\n",
    "umap_features_cluster = umap_vis.predict_umap(unique_embeddings.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_features_np = umap_features.cpu().numpy()\n",
    "umap_labels_np = umap_labels.cpu().numpy()\n",
    "umap_features_cluster_np = umap_features_cluster.cpu().numpy()\n",
    "\n",
    "# Save the UMAP features and labels\n",
    "np.save(os.path.join(store_path_0, \"umap_features.npy\"), umap_features_np)\n",
    "np.save(os.path.join(store_path_0, \"umap_labels.npy\"), umap_labels_np)\n",
    "np.save(os.path.join(store_path_0, \"umap_features_cluster.npy\"), umap_features_cluster_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loadm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_features_np = np.load(os.path.join(store_path_0, \"umap_features.npy\"), allow_pickle=True)\n",
    "umap_labels_np = np.load(os.path.join(store_path_0, \"umap_labels.npy\"), allow_pickle=True)\n",
    "umap_features_cluster_np = np.load(\n",
    "    os.path.join(store_path_0, \"umap_features_cluster.npy\"), allow_pickle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percentile not working, as its super dense in the middle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cluster_by_direction_with_representatives(\n",
    "#     umap_features,\n",
    "#     num_directions=4,\n",
    "#     percentiles=[0, 100],\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Cluster UMAP points by angular direction and return farthest & middle points per direction.\n",
    "#     Returns:\n",
    "#         direction_labels: (N,) array of direction index\n",
    "#         representatives: List of tuples (farthest_idx, middle_idx) per direction\n",
    "#     \"\"\"\n",
    "#     # Center around origin\n",
    "#     centered = umap_features - np.mean(umap_features, axis=0)\n",
    "\n",
    "#     # Compute angles and radii\n",
    "#     angles = np.arctan2(centered[:, 1], centered[:, 0])  # [-π, π]\n",
    "#     angles = (angles + 2 * np.pi) % (2 * np.pi)  # [0, 2π)\n",
    "#     radii = np.linalg.norm(centered, axis=1)\n",
    "\n",
    "#     # Assign to angular bins\n",
    "#     angle_bins = np.linspace(0, 2 * np.pi, num_directions + 1)\n",
    "#     direction_labels = np.digitize(angles, angle_bins) - 1  # 0-based\n",
    "\n",
    "#     percentile_indices = []\n",
    "\n",
    "#     for i in range(num_directions):\n",
    "#         idx_in_bin = np.where(direction_labels == i)[0]\n",
    "#         if len(idx_in_bin) == 0:\n",
    "#             percentile_indices.append([None] * len(percentiles))\n",
    "#             continue\n",
    "\n",
    "#         r_bin = radii[idx_in_bin]\n",
    "#         sorted_idx = idx_in_bin[np.argsort(r_bin)]  # sorted indices by distance\n",
    "\n",
    "#         reps = []\n",
    "#         for p in percentiles:\n",
    "#             # Compute percentile index (e.g., 20% → 0.2 * len)\n",
    "#             rank = int(\n",
    "#                 np.clip(np.ceil(p / 100 * len(sorted_idx)) - 1, 0, len(sorted_idx) - 1)\n",
    "#             )\n",
    "#             reps.append(sorted_idx[rank])\n",
    "\n",
    "#         percentile_indices.append(reps)\n",
    "\n",
    "#     return direction_labels, percentile_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_by_direction_with_radial_steps(umap_features, num_directions=4, n_steps=10):\n",
    "    \"\"\"\n",
    "    For each direction bin, find n points along the vector from center to the farthest point,\n",
    "    equally spaced in Euclidean distance.\n",
    "\n",
    "    Returns:\n",
    "        direction_labels: (N,) array of direction index\n",
    "        stepwise_indices: List of length `num_directions`, each item is a list of n point indices\n",
    "    \"\"\"\n",
    "    center = np.mean(umap_features, axis=0)\n",
    "    centered = umap_features - center\n",
    "\n",
    "    # Compute angle and radius\n",
    "    angles = np.arctan2(centered[:, 1], centered[:, 0])\n",
    "    angles = (angles + 2 * np.pi) % (2 * np.pi)\n",
    "    radii = np.linalg.norm(centered, axis=1)\n",
    "\n",
    "    # Bin into angular sectors\n",
    "    angle_bins = np.linspace(0, 2 * np.pi, num_directions + 1)\n",
    "    direction_labels = np.digitize(angles, angle_bins) - 1\n",
    "\n",
    "    stepwise_indices = []\n",
    "\n",
    "    for i in range(num_directions):\n",
    "        idx_in_bin = np.where(direction_labels == i)[0]\n",
    "        if len(idx_in_bin) == 0:\n",
    "            stepwise_indices.append([None] * n_steps)\n",
    "            continue\n",
    "\n",
    "        # Points in bin\n",
    "        points = centered[idx_in_bin]\n",
    "\n",
    "        # Find farthest point\n",
    "        far_idx_local = np.argmax(np.linalg.norm(points, axis=1))\n",
    "        far_point = points[far_idx_local]\n",
    "        far_idx = idx_in_bin[far_idx_local]\n",
    "\n",
    "        # Vector from center to far point (in centered coordinates)\n",
    "        vec = far_point\n",
    "        vec_norm = np.linalg.norm(vec)\n",
    "\n",
    "        if vec_norm == 0:\n",
    "            stepwise_indices.append([far_idx] * n_steps)\n",
    "            continue\n",
    "\n",
    "        # Directional unit vector\n",
    "        dir_vec = vec / vec_norm\n",
    "\n",
    "        # Step distances\n",
    "        step_dists = np.linspace(0, vec_norm, n_steps + 1)[1:]  # exclude 0\n",
    "\n",
    "        # For each step, find closest point to the target step location\n",
    "        reps = []\n",
    "        for d in step_dists:\n",
    "            target = d * dir_vec\n",
    "            dists_to_target = np.linalg.norm(points - target, axis=1)\n",
    "            closest_idx_local = np.argmin(dists_to_target)\n",
    "            reps.append(idx_in_bin[closest_idx_local])\n",
    "\n",
    "        stepwise_indices.append(reps)\n",
    "\n",
    "    return direction_labels, stepwise_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction_labels, direction_representatives_idx = cluster_by_direction_with_radial_steps(\n",
    "    umap_features_np, num_directions=4, n_steps=10\n",
    ")\n",
    "direction_representatives_idx = np.array(direction_representatives_idx).flatten()\n",
    "direction_representatives = umap_features_np[direction_representatives_idx]\n",
    "print(direction_representatives.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_number = 20\n",
    "kmeans = KMeans(n_clusters=min(kmeans_number, umap_features_cluster_np.shape[0])).fit(\n",
    "    umap_features_cluster_np\n",
    ")\n",
    "centroids = kmeans.cluster_centers_\n",
    "# Find closest real embedding to each centroid\n",
    "\n",
    "indices = np.argmin(cdist(centroids, umap_features_cluster_np), axis=1)\n",
    "representatives = umap_features_cluster_np[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_umap(umap_features_np, direction_labels, umap_features_cluster_np, representatives)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_umap(None, direction_labels, umap_features_cluster_np, representatives)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_umap(\n",
    "    None,\n",
    "    direction_labels,\n",
    "    umap_features_cluster_np,\n",
    "    direction_representatives,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_embeddings = label_embedding[direction_representatives_idx]\n",
    "# unique_embeddings = label_embedding[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXP 2: Test all labels and focus visualization of a single image / text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"##########Testing test dataset##########\")\n",
    "(\n",
    "    img_emb,\n",
    "    txt_emb,\n",
    "    txt_full,\n",
    "    text_to_image_map,\n",
    "    image_to_text_map,\n",
    "    inds_raw_tti,\n",
    "    inds_raw_itt,\n",
    ") = encode_data(\n",
    "    model,\n",
    "    processor,\n",
    "    test_dataloader,\n",
    "    device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go with itt experiments through all labels for a single image\n",
    "inds_tti_all = []\n",
    "mask_tti_all = []\n",
    "inds_itt_all = []\n",
    "mask_itt_all = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_id = 0\n",
    "for idx, selected_label in enumerate(tqdm(unique_embeddings)):\n",
    "    (\n",
    "        _,\n",
    "        ints_tti,\n",
    "        mask_tti,\n",
    "        inds_itt,\n",
    "        mask_itt,\n",
    "    ) = eval_rank_oracle_check_per_label(\n",
    "        model,\n",
    "        selected_label,\n",
    "        img_emb,\n",
    "        txt_emb,\n",
    "        txt_full,\n",
    "        text_to_image_map,\n",
    "        image_to_text_map,\n",
    "        inds_raw_tti=inds_raw_tti,\n",
    "        inds_raw_itt=inds_raw_itt,\n",
    "    )\n",
    "    inds_tti_all.append(ints_tti.detach().cpu().numpy().astype(np.uint16))\n",
    "    mask_tti_all.append(mask_tti.detach().cpu().numpy().astype(np.uint16))\n",
    "    inds_itt_all.append(inds_itt.detach().cpu().numpy().astype(np.uint16))\n",
    "    mask_itt_all.append(mask_itt.detach().cpu().numpy().astype(np.uint16))\n",
    "\n",
    "    # comb_emb = comb_emb.detach().cpu().numpy()\n",
    "    # torch.save(\n",
    "    #     comb_emb,\n",
    "    #     f\"{store_path_2}/comb_emb_{idx}.pt\",\n",
    "    # )\n",
    "    # Save the buffer every 10 iterations\n",
    "    if (idx + 1) % 10 == 0 or idx == len(unique_embeddings) - 1:\n",
    "        np.save(\n",
    "            f\"{store_path}/inds_tti_all_{save_id}.npy\",\n",
    "            np.array(inds_tti_all).astype(np.uint16),\n",
    "        )\n",
    "        np.save(\n",
    "            f\"{store_path}/mask_tti_all_{save_id}.npy\",\n",
    "            np.array(mask_tti_all).astype(np.uint16),\n",
    "        )\n",
    "        np.save(\n",
    "            f\"{store_path}/inds_itt_all_{save_id}.npy\",\n",
    "            np.array(inds_itt_all).astype(np.uint16),\n",
    "        )\n",
    "        np.save(\n",
    "            f\"{store_path}/mask_itt_all_{save_id}.npy\",\n",
    "            np.array(mask_itt_all).astype(np.uint16),\n",
    "        )\n",
    "        save_id += 1\n",
    "        # clear the buffer\n",
    "        inds_tti_all.clear()\n",
    "        mask_tti_all.clear()\n",
    "        inds_itt_all.clear()\n",
    "        mask_itt_all.clear()\n",
    "\n",
    "\"\"\"\n",
    "1. inds_itt is the indices of the itt inds using combined embeddings\n",
    "2. mask_itt is the mask of the image-shape that indicate which image improved by using the selected label.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sorted_npy_files(folder_path: str, prefix: str) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Load .npy files matching the pattern {prefix}_{idx}.npy in numeric order.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the directory containing .npy files.\n",
    "        prefix (str): Prefix of the filename before the index, e.g., 'xxx' for 'xxx_1.npy'.\n",
    "\n",
    "    Returns:\n",
    "        List[np.ndarray]: List of loaded numpy arrays sorted by their index.\n",
    "    \"\"\"\n",
    "    pattern = re.compile(rf\"{re.escape(prefix)}_(\\d+)\\.npy$\")\n",
    "\n",
    "    file_list = sorted(\n",
    "        [f for f in os.listdir(folder_path) if pattern.match(f)],\n",
    "        key=lambda x: int(pattern.match(x).group(1)),  # type: ignore\n",
    "    )\n",
    "\n",
    "    output = [np.load(os.path.join(folder_path, fname)) for fname in file_list]\n",
    "    output = np.concatenate(output, axis=0)\n",
    "\n",
    "    return output  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved files\n",
    "inds_tti_all = load_sorted_npy_files(store_path, \"inds_tti_all\")\n",
    "mask_tti_all = load_sorted_npy_files(store_path, \"mask_tti_all\")\n",
    "inds_itt_all = load_sorted_npy_files(store_path, \"inds_itt_all\")\n",
    "mask_itt_all = load_sorted_npy_files(store_path, \"mask_itt_all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"inds_tti_all shape: {inds_tti_all.shape}, mask_tti_all shape: {mask_tti_all.shape}\")\n",
    "print(f\"inds_itt_all shape: {inds_itt_all.shape}, mask_itt_all shape: {mask_itt_all.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization itt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_path = cfg.dataset.test_path\n",
    "img_path = cfg.dataset.img_path_test\n",
    "\n",
    "ann = json.load(open(ann_path, \"r\"))\n",
    "txt_collection = [item[\"caption\"] for item in ann]\n",
    "\n",
    "if type(txt_collection[0]) is not str:\n",
    "    txt_collection = [item for sublist in txt_collection for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## itt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_index = 0  # Which image to choose to visualize\n",
    "mask_itt_indices = 0  # Which label to choose to visualize\n",
    "only_see_improved = False  # Only see mask_itt_indices if it improved the image\n",
    "check_top_k = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from scipy.stats import entropy  # Computes KL divergence\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def compute_similarity_distribution(text_feats, image_feat, temperature=0.07):\n",
    "    sims = cosine_similarity(text_feats, image_feat.reshape(1, -1)).squeeze()\n",
    "    return softmax(sims / temperature)\n",
    "\n",
    "\n",
    "def compute_kl_divergence(text_feats_1, text_feats_2, image_feat, temperature=0.07):\n",
    "    # Convert both to probability distributions over the k retrieved texts\n",
    "    P = compute_similarity_distribution(text_feats_1, image_feat, temperature)\n",
    "    Q = compute_similarity_distribution(text_feats_2, image_feat, temperature)\n",
    "\n",
    "    # Add small epsilon to avoid log(0) issues\n",
    "    epsilon = 1e-8\n",
    "    P = np.clip(P, epsilon, 1.0)\n",
    "    Q = np.clip(Q, epsilon, 1.0)\n",
    "\n",
    "    kl_PQ = entropy(P, Q)  # KL(P || Q)\n",
    "    kl_QP = entropy(Q, P)  # KL(Q || P)\n",
    "    jsd = 0.5 * (kl_PQ + kl_QP)\n",
    "\n",
    "    return {\"KL(P||Q)\": kl_PQ.item(), \"KL(Q||P)\": kl_QP.item(), \"JSD\": jsd.item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_index_slider = widgets.IntSlider(\n",
    "    value=image_index, min=0, max=len(ann) - 1, step=1, description=\"Image Index\"\n",
    ")\n",
    "mask_itt_slider = widgets.IntSlider(\n",
    "    value=mask_itt_indices,\n",
    "    min=0,\n",
    "    max=len(unique_embeddings),\n",
    "    step=1,\n",
    "    description=\"Label Index\",\n",
    ")\n",
    "only_improved_checkbox = widgets.Checkbox(\n",
    "    value=only_see_improved, description=\"Only Show Improved\"\n",
    ")\n",
    "top_k_slider = widgets.IntSlider(value=check_top_k, min=1, max=50, step=1, description=\"Top K\")\n",
    "\n",
    "\n",
    "def update_visualization(image_index, mask_itt_indices, only_see_improved, check_top_k):\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    inds_itt_per_image = [inds_itt[image_index] for inds_itt in inds_itt_all]\n",
    "    mask_itt_per_image = [mask_itt[image_index] for mask_itt in mask_itt_all]\n",
    "\n",
    "    # Make sure that image_index and mask_itt_indices are within the range\n",
    "    image_index = min(image_index, len(img_emb) - 1)\n",
    "    mask_itt_indices = min(mask_itt_indices, len(mask_itt_per_image) - 1)\n",
    "\n",
    "    # Find the True values in mask_itt and its corresponding indices\n",
    "    mask_itt_indices_true = np.where(mask_itt_per_image)[0]\n",
    "    # print(\n",
    "    #     f\"Image is improved with at least one label: {len(mask_itt_indices_true) > 0}\"\n",
    "    # )\n",
    "    # print(\n",
    "    #     f\"Number of labels that improved the image: {len(mask_itt_indices_true)} out of {len(mask_itt_per_image)}\"\n",
    "    # )\n",
    "    improved = mask_itt_per_image[mask_itt_indices]\n",
    "\n",
    "    if only_see_improved and improved:\n",
    "        mask_itt_indices = mask_itt_indices_true[\n",
    "            min(mask_itt_indices, len(mask_itt_indices_true) - 1)\n",
    "        ]\n",
    "\n",
    "    # Apply filtering\n",
    "    if only_see_improved and not improved:\n",
    "        print(\"No improvement for this image.\")\n",
    "        return\n",
    "\n",
    "    # Get item\n",
    "    item = ann[image_index]\n",
    "\n",
    "    img = os.path.join(img_path, item[\"image\"])\n",
    "    img = Image.open(img).convert(\"RGB\")\n",
    "    image_feature = img_emb[image_index]\n",
    "\n",
    "    # turn off axis\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Improved\" if improved else \"Not Improved\")\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "    # First get original caption\n",
    "    original_caption = item[\"caption\"]\n",
    "\n",
    "    # Then get the caption retrieved by raw\n",
    "    retrived_caption_index_raw = inds_raw_itt[image_index][:check_top_k]\n",
    "    retrived_caption_raw = [txt_collection[i] for i in retrived_caption_index_raw]\n",
    "    retrived_caption_raw_features = txt_emb[retrived_caption_index_raw]\n",
    "\n",
    "    # Finally get the caption retrieved by our method\n",
    "    retrived_caption_index_cdc = inds_itt_per_image[mask_itt_indices][:check_top_k].tolist()\n",
    "    retrived_caption_cdc = [txt_collection[i] for i in retrived_caption_index_cdc]\n",
    "    # retrived_caption_cdc_features = torch.load(\n",
    "    #     f\"{store_path}/comb_emb_{mask_itt_indices}.pt\", weights_only=False\n",
    "    # )[retrived_caption_index_cdc]\n",
    "    retrived_caption_cdc_features = txt_emb[retrived_caption_index_cdc]\n",
    "\n",
    "    # Compute KL divergence\n",
    "    result = compute_kl_divergence(\n",
    "        retrived_caption_raw_features, retrived_caption_cdc_features, image_feature\n",
    "    )\n",
    "    print(result)\n",
    "\n",
    "    # Turn into a panda dataframe\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"Raw_retrieve\": retrived_caption_raw,\n",
    "            \"CDC_retrieve\": retrived_caption_cdc,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Function to highlight duplicates\n",
    "    def highlight_duplicates(val, col1, col2):\n",
    "        # If the value appears in both columns, color it blue\n",
    "\n",
    "        if val in original_caption:\n",
    "            return \"background-color: lightgreen\"\n",
    "\n",
    "        if val in df[col1].values and val in df[col2].values:\n",
    "            return \"background-color: lightblue\"\n",
    "\n",
    "        return \"\"\n",
    "\n",
    "    # Display the dataframe\n",
    "    with pd.option_context(\"display.max_colwidth\", None):\n",
    "        styled_df = df.style.map(highlight_duplicates, col1=\"Raw_retrieve\", col2=\"CDC_retrieve\")\n",
    "        display(styled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive UI\n",
    "ui = widgets.VBox([image_index_slider, mask_itt_slider, only_improved_checkbox, top_k_slider])\n",
    "\n",
    "out = widgets.Output()\n",
    "\n",
    "\n",
    "def on_change(change):\n",
    "    with out:\n",
    "        update_visualization(\n",
    "            image_index_slider.value,\n",
    "            mask_itt_slider.value,\n",
    "            only_improved_checkbox.value,\n",
    "            top_k_slider.value,\n",
    "        )\n",
    "\n",
    "\n",
    "image_index_slider.observe(on_change, names=\"value\")\n",
    "mask_itt_slider.observe(on_change, names=\"value\")\n",
    "only_improved_checkbox.observe(on_change, names=\"value\")\n",
    "top_k_slider.observe(on_change, names=\"value\")\n",
    "\n",
    "update_visualization(image_index, mask_itt_indices, only_see_improved, check_top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ui, out)  # 2026"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divergence Access\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tti\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_index = 10  # Which image to choose to visualize\n",
    "mask_tti_indices = 0  # Which label to choose to visualize\n",
    "only_see_improved = False  # Only see mask_tti_indices if it improved the image\n",
    "check_top_k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_index_slider = widgets.IntSlider(\n",
    "    value=text_index, min=0, max=(len(ann) - 1) * 5, step=1, description=\"Text Index\"\n",
    ")\n",
    "mask_tti_slider = widgets.IntSlider(\n",
    "    value=mask_tti_indices,\n",
    "    min=0,\n",
    "    max=len(unique_embeddings),\n",
    "    step=1,\n",
    "    description=\"Label Index\",\n",
    ")\n",
    "only_improved_checkbox = widgets.Checkbox(\n",
    "    value=only_see_improved, description=\"Only Show Improved\"\n",
    ")\n",
    "top_k_slider = widgets.IntSlider(value=check_top_k, min=1, max=50, step=1, description=\"Top K\")\n",
    "\n",
    "\n",
    "def update_visualization2(text_index, mask_tti_indices, only_see_improved, check_top_k):\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    inds_tti_per_text = [inds_tti[text_index] for inds_tti in inds_tti_all]\n",
    "    mask_tti_per_text = [mask_tti[text_index] for mask_tti in mask_tti_all]\n",
    "\n",
    "    # Make sure that image_index and mask_itt_indices are within the range\n",
    "    text_index = min(text_index, len(txt_emb) - 1)\n",
    "    mask_tti_indices = min(mask_tti_indices, len(mask_tti_per_text) - 1)\n",
    "\n",
    "    # Find the True values in mask_itt and its corresponding indices\n",
    "    mask_tti_indices_true = np.where(mask_tti_per_text)[0]\n",
    "    improved = mask_tti_per_text[mask_tti_indices]\n",
    "\n",
    "    if only_see_improved and improved:\n",
    "        mask_tti_indices = mask_tti_indices_true[\n",
    "            min(mask_tti_indices, len(mask_tti_indices_true) - 1)\n",
    "        ]\n",
    "\n",
    "    # Apply filtering\n",
    "    if only_see_improved and not improved:\n",
    "        print(\"No improvement for this text.\")\n",
    "        return\n",
    "\n",
    "    query_caption = txt_collection[text_index]\n",
    "    print(f\"Query Caption: {query_caption}\")\n",
    "    print(f\"Improved: {improved}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Get item\n",
    "    raw_indices = inds_raw_tti[text_index][:check_top_k]\n",
    "    cdc_indices = inds_tti_per_text[mask_tti_indices][:check_top_k].tolist()\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=check_top_k, ncols=2, figsize=(15, check_top_k * 2.5))\n",
    "    if check_top_k == 1:\n",
    "        axes = np.expand_dims(axes, axis=0)\n",
    "\n",
    "    for i in range(check_top_k):\n",
    "        for j, (indices, label) in enumerate(zip([raw_indices, cdc_indices], [\"Raw\", \"CDC\"])):\n",
    "            ax = axes[i, j]\n",
    "            img_id = indices[i]\n",
    "            item = ann[img_id]\n",
    "            img_path_full = os.path.join(img_path, item[\"image\"])\n",
    "            img = Image.open(img_path_full).convert(\"RGB\")\n",
    "            ax.imshow(img)\n",
    "            ax.axis(\"off\")\n",
    "            ax.set_title(f\"{label} Top-{i+1}: {item['caption']}\", fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive UI\n",
    "ui = widgets.VBox([text_index_slider, mask_tti_slider, only_improved_checkbox, top_k_slider])\n",
    "\n",
    "out = widgets.Output()\n",
    "\n",
    "\n",
    "def on_change(change):\n",
    "    with out:\n",
    "        update_visualization2(\n",
    "            text_index_slider.value,\n",
    "            mask_tti_slider.value,\n",
    "            only_improved_checkbox.value,\n",
    "            top_k_slider.value,\n",
    "        )\n",
    "\n",
    "\n",
    "text_index_slider.observe(on_change, names=\"value\")\n",
    "mask_tti_slider.observe(on_change, names=\"value\")\n",
    "only_improved_checkbox.observe(on_change, names=\"value\")\n",
    "top_k_slider.observe(on_change, names=\"value\")\n",
    "\n",
    "update_visualization2(text_index, mask_tti_indices, only_see_improved, check_top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ui, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepclustering2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
