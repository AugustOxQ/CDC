{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.imp_datamodule import IMP_train\n",
    "from src.utils.clustering_utils import *\n",
    "from src.models.components.clustering import get_umap, get_kmeans\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageFile\n",
    "\n",
    "import hydra\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from torch.optim.lr_scheduler import (\n",
    "    CosineAnnealingLR,\n",
    ")\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoImageProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMP_train(Dataset):\n",
    "    def __init__(self, annotation_path, image_path, preprocess, ratio=0.1):\n",
    "        self.annotations = json.load(open(annotation_path))\n",
    "        self.annotations = self.annotations[: int(len(self.annotations) * ratio)]\n",
    "        self.image_path = image_path\n",
    "        self.vis_processors = preprocess\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        raw_image = Image.open(\n",
    "            os.path.join(self.image_path, self.annotations[idx][\"image\"])\n",
    "        ).convert(\"RGB\")\n",
    "        image_input = self.vis_processors(raw_image, return_tensors=\"pt\")\n",
    "        if \"pixel_values\" in image_input:\n",
    "            image_input[\"pixel_values\"] = image_input[\"pixel_values\"].squeeze()\n",
    "\n",
    "        raw_text = self.annotations[idx][\"caption\"]\n",
    "\n",
    "        return image_input, raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "preprocess = AutoImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "train_dataset = IMP_train(\n",
    "        annotation_path=\"/data/SSD/coco/annotations/coco_karpathy_train.json\",\n",
    "        image_path=\"/data/SSD/coco/images\",\n",
    "        preprocess=preprocess,\n",
    "        ratio=1,\n",
    "    )\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_id, batch in enumerate(train_dataloader):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepclustering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
