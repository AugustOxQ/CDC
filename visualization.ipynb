{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from tabnanny import verbose\n",
    "from turtle import update\n",
    "\n",
    "import hydra\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "from hydra import compose, initialize, initialize_config_dir, initialize_config_module\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from sympy import count_ops, use\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoImageProcessor, AutoProcessor, AutoTokenizer\n",
    "\n",
    "# Import local packages\n",
    "from src.data.cdc_datamodule import CDC_test\n",
    "from src.models.cdc import CDC\n",
    "from src.models.components.clustering import Clustering, UMAP_vis\n",
    "from src.utils import (\n",
    "    print_model_info,\n",
    ")\n",
    "from src.utils.evaltools import eval_rank_oracle_check_per_label\n",
    "from src.utils.inference import encode_data, inference_test\n",
    "\n",
    "# Setup\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "transformers.logging.set_verbosity_error()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize Hydra\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "initialize(config_path=\"configs\", version_base=None)\n",
    "cfg = compose(config_name=\"flickr30k\")\n",
    "print(*cfg, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "\n",
    "from eval import main\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)  # Ensures full text is shown\n",
    "pd.set_option(\"display.max_rows\", 200)  # Increase max rows if needed\n",
    "pd.set_option(\"display.max_columns\", 50)  # Increase max columns if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "Select the best label for each image and text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This corresponds to the main() function in the original code: eval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "seed = cfg.seed\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Define the parent folder\n",
    "parent_folder = \"res\"\n",
    "\n",
    "res_path = \"/project/Deep-Clustering/res/20250219_043513_mscoco-preextracted\"\n",
    "\n",
    "if res_path is None:\n",
    "    print(\"No path provided. Searching for the latest experiment...\")\n",
    "    # Get a list of all subdirectories inside the parent folder\n",
    "    subfolders = [\n",
    "        os.path.join(parent_folder, d)\n",
    "        for d in os.listdir(parent_folder)\n",
    "        if os.path.isdir(os.path.join(parent_folder, d))\n",
    "    ]\n",
    "\n",
    "    # Sort subfolders by modification time (newest first)\n",
    "    res_path = max(subfolders, key=os.path.getmtime) if subfolders else None\n",
    "\n",
    "print(f\"Using results from: {res_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_best_label = True\n",
    "\n",
    "# Initialize Model\n",
    "model = CDC(\n",
    "    clip_trainable=False,\n",
    "    d_model=cfg.model.d_model,\n",
    "    nhead=cfg.model.num_heads,\n",
    "    num_layers=cfg.model.num_layers,\n",
    "    label_dim=cfg.model.label_dim,\n",
    ")\n",
    "model = nn.DataParallel(model)\n",
    "# load model\n",
    "model.load_state_dict(torch.load(f\"{res_path}/final_model.pth\"))\n",
    "model.to(device)\n",
    "\n",
    "clustering = Clustering()\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "ann_path = cfg.dataset.test_path\n",
    "ann = json.load(open(ann_path, \"r\"))\n",
    "\n",
    "if len(ann) > 5000:\n",
    "    ratio = 5000 / len(ann)\n",
    "else:\n",
    "    ratio = 1\n",
    "\n",
    "del ann_path, ann\n",
    "\n",
    "test_dataset = CDC_test(\n",
    "    annotation_path=cfg.dataset.test_path,\n",
    "    image_path=cfg.dataset.img_path_test,\n",
    "    processor=processor,\n",
    "    ratio=ratio,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=cfg.eval.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=cfg.train.num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXP 1: Find labels among all possible choices and select the best one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"##########Testing test dataset##########\")\n",
    "# unique_embeddings = torch.load(f\"{res_path}/unique_embeddings.pt\")\n",
    "# (\n",
    "#     img_emb,\n",
    "#     txt_emb,\n",
    "#     txt_full,\n",
    "#     text_to_image_map,\n",
    "#     image_to_text_map,\n",
    "#     best_label_tti,\n",
    "#     best_label_itt,\n",
    "#     inds_raw_tti,\n",
    "#     inds_raw_itt,\n",
    "# ) = inference_test(\n",
    "#     model,\n",
    "#     processor,\n",
    "#     test_dataloader,\n",
    "#     unique_embeddings,\n",
    "#     -1,\n",
    "#     device,\n",
    "#     inspect_labels=True,\n",
    "#     use_best_label=use_best_label,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of original image and text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # First use img_emb + txt_emb to create a cat_emb and compute umap\n",
    "# cat_emb = torch.cat((img_emb, txt_emb), dim=0)\n",
    "\n",
    "# umap_vis = UMAP_vis()\n",
    "# umap_features = umap_vis.learn_umap(cat_emb, n_components=2)\n",
    "# umap_features_raw_image = umap_features[: img_emb.shape[0], :]\n",
    "# umap_features_raw_text = umap_features[img_emb.shape[0] :, :]\n",
    "\n",
    "# # Compute original umap\n",
    "# fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "# # Plot image embeddings\n",
    "# plt.scatter(\n",
    "#     umap_features[: img_emb.shape[0], 0],\n",
    "#     umap_features[: img_emb.shape[0], 1],\n",
    "#     s=5,\n",
    "#     alpha=1,\n",
    "#     label=\"Image Embeddings\",\n",
    "# )\n",
    "\n",
    "# # Plot text embeddings\n",
    "# plt.scatter(\n",
    "#     umap_features[img_emb.shape[0] : img_emb.shape[0] + txt_emb.shape[0], 0],\n",
    "#     umap_features[img_emb.shape[0] : img_emb.shape[0] + txt_emb.shape[0], 1],\n",
    "#     s=5,\n",
    "#     alpha=1,\n",
    "#     label=\"Text Embeddings\",\n",
    "# )\n",
    "\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.xlabel(\"UMAP 1\")\n",
    "# plt.ylabel(\"UMAP 2\")\n",
    "# str_tag = \"raw\"\n",
    "# plt.show()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### itt part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # itt label selections\n",
    "# # Find unique values in best_label\n",
    "# unique_values_label_itt, counts_label_itt = torch.unique(best_label_itt, return_counts=True)\n",
    "# unique_values_label_itt = unique_values_label_itt[unique_values_label_itt != -1]\n",
    "\n",
    "# N = len(unique_values_label_itt)\n",
    "\n",
    "# print(f\"##########Evaluating top {N} labels##########\")\n",
    "# selected_label_indices_itt = int(unique_values_label_itt[0])\n",
    "# selected_label_itt = unique_embeddings[selected_label_indices_itt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Go with itt experiments\n",
    "# comb_emb_itt, inds_itt, mask_itt = eval_rank_oracle_check_per_label(\n",
    "#     model,\n",
    "#     selected_label_itt,\n",
    "#     True,  # True means go with itt experiments\n",
    "#     img_emb,\n",
    "#     txt_emb,\n",
    "#     txt_full,\n",
    "#     text_to_image_map,\n",
    "#     image_to_text_map,\n",
    "#     inds_raw_tti=inds_raw_tti,\n",
    "#     inds_raw_itt=inds_raw_itt,\n",
    "# )\n",
    "\n",
    "# \"\"\"\n",
    "# 1. Comb_emb_itt is the combined embeddings of all texts and the selected label\n",
    "# 2. inds_itt is the indices of the itt inds using combined embeddings\n",
    "# 3. mask_itt is the mask of the image-shape that indicate which image improved by using the selected label\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot UMAP for all points\n",
    "# fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "# comb_emb_itt_np = comb_emb_itt.detach().cpu().numpy()\n",
    "# umap_features_new_itt = umap_vis.predict_umap(comb_emb_itt_np)\n",
    "\n",
    "# # Compute original umap\n",
    "# fig = plt.figure(figsize=(10, 10))\n",
    "# # Plot image embeddings\n",
    "# plt.scatter(\n",
    "#     umap_features_raw_image[:, 0],\n",
    "#     umap_features_raw_image[:, 1],\n",
    "#     s=5,\n",
    "#     alpha=1,\n",
    "#     label=\"Image Embeddings\",\n",
    "# )\n",
    "\n",
    "# # Plot text embeddings\n",
    "# plt.scatter(\n",
    "#     umap_features_raw_text[:, 0],\n",
    "#     umap_features_raw_text[:, 1],\n",
    "#     s=5,\n",
    "#     alpha=1,\n",
    "#     label=\"Text Embeddings\",\n",
    "# )\n",
    "\n",
    "# # Plot combined embeddings\n",
    "# plt.scatter(\n",
    "#     umap_features_new_itt[:, 0],\n",
    "#     umap_features_new_itt[:, 1],\n",
    "#     s=5,\n",
    "#     alpha=1,\n",
    "#     label=\"Combined Embeddings\",\n",
    "# )\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "\n",
    "# plt.xlabel(\"UMAP 1\")\n",
    "# plt.ylabel(\"UMAP 2\")\n",
    "# str_tag = \"best\" if use_best_label else \"first\"\n",
    "# plt.show()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tti part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tti label selections\n",
    "# # Find unique values in best_label\n",
    "# unique_values_label_tti, counts_label_tti = torch.unique(best_label_tti, return_counts=True)\n",
    "# unique_values_label_tti = unique_values_label_tti[unique_values_label_tti != -1]\n",
    "\n",
    "# N = len(unique_values_label_tti)\n",
    "\n",
    "# print(f\"##########Evaluating top {N} labels##########\")\n",
    "# selected_label_indices_tti = int(unique_values_label_tti[0])\n",
    "# selected_label_tti = unique_embeddings[selected_label_indices_tti]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Go with tti experiments\n",
    "# comb_emb_tti, inds_tti, mask_tti, inds_tti, mask_tti = eval_rank_oracle_check_per_label(\n",
    "#     model,\n",
    "#     selected_label_tti,\n",
    "#     img_emb,\n",
    "#     txt_emb,\n",
    "#     txt_full,\n",
    "#     text_to_image_map,\n",
    "#     image_to_text_map,\n",
    "#     inds_raw_tti=inds_raw_tti,\n",
    "#     inds_raw_itt=inds_raw_itt,\n",
    "# )\n",
    "\n",
    "# \"\"\"\n",
    "# 1. Comb_emb_tti is the combined embeddings of all images and the selected label\n",
    "# 2. inds_tti is the indices of the tti inds using combined embeddings\n",
    "# 3. mask_tti is the mask of the image-shape that indicate which image improved by using the selected label\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot itt UMAP for improved points only\n",
    "# fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "# # Plot image embeddings\n",
    "# plt.scatter(\n",
    "#     umap_features_raw_image[mask_itt, 0],\n",
    "#     umap_features_raw_image[mask_itt, 1],\n",
    "#     s=5,\n",
    "#     alpha=1,\n",
    "#     label=\"Image Embeddings (Improved)\",\n",
    "# )\n",
    "\n",
    "# # Plot text embeddings\n",
    "# plt.scatter(\n",
    "#     umap_features_raw_text[\n",
    "#         mask_tti, 0\n",
    "#     ],  # TODO this is not actually mask_tti but rather expanded mask_tti\n",
    "#     umap_features_raw_text[mask_tti, 1],\n",
    "#     s=5,\n",
    "#     alpha=1,\n",
    "#     label=\"Text Embeddings (Improved)\",\n",
    "# )\n",
    "\n",
    "# # Plot combined embeddings\n",
    "# plt.scatter(\n",
    "#     umap_features_new_itt[mask_tti, 0],\n",
    "#     umap_features_new_itt[mask_tti, 1],\n",
    "#     s=5,\n",
    "#     alpha=1,\n",
    "#     label=\"Combined Embeddings (Improved)\",\n",
    "# )\n",
    "\n",
    "# # Plot image embeddings\n",
    "# plt.scatter(\n",
    "#     umap_features_raw_image[~mask_itt, 0],\n",
    "#     umap_features_raw_image[~mask_itt, 1],\n",
    "#     s=5,\n",
    "#     alpha=0.05,\n",
    "#     color=\"blue\",\n",
    "# )\n",
    "\n",
    "# # Plot text embeddings\n",
    "# plt.scatter(\n",
    "#     umap_features_raw_text[~mask_itt_expand, 0],\n",
    "#     umap_features_raw_text[~mask_itt_expand, 1],\n",
    "#     s=5,\n",
    "#     alpha=0.05,\n",
    "#     color=\"orange\",\n",
    "# )\n",
    "\n",
    "# # Plot combined embeddings\n",
    "# plt.scatter(\n",
    "#     umap_features_new_itt[~mask_itt_expand, 0],\n",
    "#     umap_features_new_itt[~mask_itt_expand, 1],\n",
    "#     s=5,\n",
    "#     alpha=0.05,\n",
    "#     color=\"green\",\n",
    "# )\n",
    "\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "\n",
    "# plt.xlabel(\"UMAP 1\")\n",
    "# plt.ylabel(\"UMAP 2\")\n",
    "# str_tag = \"best\" if use_best_label else \"first\"\n",
    "# plt.show()\n",
    "# plt.close()\n",
    "\n",
    "# umap_vis.close_cluster()\n",
    "\n",
    "# # Clean cuda cache\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization itt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ann_path = cfg.dataset.test_path\n",
    "# img_path = cfg.dataset.img_path_test\n",
    "\n",
    "# ann = json.load(open(ann_path, \"r\"))\n",
    "# txt_collection = [item[\"caption\"] for item in ann]\n",
    "# txt_collection = [item for sublist in txt_collection for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find the True values in mask_itt and its corresponding indices\n",
    "# mask_itt_indices = np.where(mask_itt)[0]\n",
    "# mask_itt_indices[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 2\n",
    "# i = min(i, len(mask_itt_indices) - 1)\n",
    "\n",
    "# idx = mask_itt_indices[i]  # idx is the true index of the image in ann that improved\n",
    "# check_top_k = 50\n",
    "# item = ann[idx]\n",
    "\n",
    "# img = os.path.join(img_path, item[\"image\"])\n",
    "# img = Image.open(img).convert(\"RGB\")\n",
    "\n",
    "# # turn off axis\n",
    "# plt.axis(\"off\")\n",
    "# plt.imshow(img)\n",
    "# plt.show()\n",
    "\n",
    "# # First get original caption\n",
    "# original_caption = item[\"caption\"]\n",
    "\n",
    "# # Then get the caption retrieved by raw\n",
    "# retrived_caption_index_raw = inds_raw_itt[idx][:check_top_k]\n",
    "# retrived_caption_raw = [txt_collection[i] for i in retrived_caption_index_raw]\n",
    "\n",
    "# # Finally get the caption retrieved by our method\n",
    "# retrived_caption_index_cdc = inds_itt[idx][:check_top_k].tolist()\n",
    "# retrived_caption_cdc = [txt_collection[i] for i in retrived_caption_index_cdc]\n",
    "\n",
    "# # Turn into a panda dataframe\n",
    "# df = pd.DataFrame(\n",
    "#     {\n",
    "#         \"Raw_retrieve\": retrived_caption_raw,\n",
    "#         \"CDC_retrieve\": retrived_caption_cdc,\n",
    "#     }\n",
    "# )\n",
    "\n",
    "\n",
    "# # Function to highlight duplicates\n",
    "# def highlight_duplicates(val, col1, col2):\n",
    "#     # If the value appears in both columns, color it blue\n",
    "\n",
    "#     if val in original_caption:\n",
    "#         return \"background-color: lightgreen\"\n",
    "\n",
    "#     if val in df[col1].values and val in df[col2].values:\n",
    "#         return \"background-color: lightblue\"\n",
    "\n",
    "#     return \"\"\n",
    "\n",
    "\n",
    "# # Display the dataframe\n",
    "# with pd.option_context(\"display.max_colwidth\", None):\n",
    "#     styled_df = df.style.map(highlight_duplicates, col1=\"Raw_retrieve\", col2=\"CDC_retrieve\")\n",
    "#     display(styled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXP 2: Test all labels and focus visualization of a single image / text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"##########Testing test dataset##########\")\n",
    "unique_embeddings = torch.load(f\"{res_path}/unique_embeddings.pt\")\n",
    "(\n",
    "    img_emb,\n",
    "    txt_emb,\n",
    "    txt_full,\n",
    "    text_to_image_map,\n",
    "    image_to_text_map,\n",
    "    inds_raw_tti,\n",
    "    inds_raw_itt,\n",
    ") = encode_data(\n",
    "    model,\n",
    "    processor,\n",
    "    test_dataloader,\n",
    "    device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go with itt experiments through all labels for a single image\n",
    "inds_itt_all = []\n",
    "mask_itt_all = []\n",
    "for selected_label in tqdm(unique_embeddings):\n",
    "    (\n",
    "        _,\n",
    "        _,\n",
    "        _,\n",
    "        inds_itt,\n",
    "        mask_itt,\n",
    "    ) = eval_rank_oracle_check_per_label(\n",
    "        model,\n",
    "        selected_label,\n",
    "        img_emb,\n",
    "        txt_emb,\n",
    "        txt_full,\n",
    "        text_to_image_map,\n",
    "        image_to_text_map,\n",
    "        inds_raw_tti=inds_raw_tti,\n",
    "        inds_raw_itt=inds_raw_itt,\n",
    "    )\n",
    "\n",
    "    inds_itt_all.append(inds_itt)\n",
    "    mask_itt_all.append(mask_itt)\n",
    "\n",
    "\"\"\"\n",
    "1. inds_itt is the indices of the itt inds using combined embeddings\n",
    "2. mask_itt is the mask of the image-shape that indicate which image improved by using the selected label.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization itt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_path = cfg.dataset.test_path\n",
    "img_path = cfg.dataset.img_path_test\n",
    "\n",
    "ann = json.load(open(ann_path, \"r\"))\n",
    "txt_collection = [item[\"caption\"] for item in ann]\n",
    "txt_collection = [item for sublist in txt_collection for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_index = 5  # Which image to choose to visualize\n",
    "mask_itt_indices = 5  # Which label to choose to visualize\n",
    "only_see_improved = True  # Only see mask_itt_indices if it improved the image\n",
    "\n",
    "inds_itt_per_image = [inds_itt[image_index] for inds_itt in inds_itt_all]\n",
    "mask_itt_per_image = [mask_itt[image_index] for mask_itt in mask_itt_all]\n",
    "\n",
    "# Make sure that image_index and mask_itt_indices are within the range\n",
    "image_index = min(image_index, len(img_emb) - 1)\n",
    "mask_itt_indices = min(mask_itt_indices, len(mask_itt_per_image) - 1)\n",
    "\n",
    "# Find the True values in mask_itt and its corresponding indices\n",
    "mask_itt_indices_true = np.where(mask_itt_per_image)[0]\n",
    "print(f\"Image is improved with at least one label: {len(mask_itt_indices_true) > 0}\")\n",
    "print(\n",
    "    f\"Number of labels that improved the image: {len(mask_itt_indices_true)} out of {len(mask_itt_per_image)}\"\n",
    ")\n",
    "improved = mask_itt_per_image[mask_itt_indices]\n",
    "print(f\"Image is improved: {improved}\")\n",
    "\n",
    "if only_see_improved and improved:\n",
    "    mask_itt_indices = mask_itt_indices_true[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_top_k = 50\n",
    "item = ann[image_index]\n",
    "\n",
    "img = os.path.join(img_path, item[\"image\"])\n",
    "img = Image.open(img).convert(\"RGB\")\n",
    "\n",
    "# turn off axis\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Improved\" if improved else \"Not Improved\")\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "# First get original caption\n",
    "original_caption = item[\"caption\"]\n",
    "\n",
    "# Then get the caption retrieved by raw\n",
    "retrived_caption_index_raw = inds_raw_itt[image_index][:check_top_k]\n",
    "retrived_caption_raw = [txt_collection[i] for i in retrived_caption_index_raw]\n",
    "\n",
    "# Finally get the caption retrieved by our method\n",
    "retrived_caption_index_cdc = inds_itt_per_image[mask_itt_indices][:check_top_k].tolist()\n",
    "retrived_caption_cdc = [txt_collection[i] for i in retrived_caption_index_cdc]\n",
    "\n",
    "# Turn into a panda dataframe\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"Raw_retrieve\": retrived_caption_raw,\n",
    "        \"CDC_retrieve\": retrived_caption_cdc,\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# Function to highlight duplicates\n",
    "def highlight_duplicates(val, col1, col2):\n",
    "    # If the value appears in both columns, color it blue\n",
    "\n",
    "    if val in original_caption:\n",
    "        return \"background-color: lightgreen\"\n",
    "\n",
    "    if val in df[col1].values and val in df[col2].values:\n",
    "        return \"background-color: lightblue\"\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "# Display the dataframe\n",
    "with pd.option_context(\"display.max_colwidth\", None):\n",
    "    styled_df = df.style.map(highlight_duplicates, col1=\"Raw_retrieve\", col2=\"CDC_retrieve\")\n",
    "    display(styled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepclustering2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
